{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/root/nn-expt/.venv/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/root/nn-expt/.venv/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory ./tuple-autoencoder/qw69yb8j/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type      | Params | Mode \n",
      "------------------------------------------------\n",
      "0 | embedding | Embedding | 200    | train\n",
      "1 | linear    | Linear    | 2.1 K  | train\n",
      "------------------------------------------------\n",
      "2.3 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.3 K     Total params\n",
      "0.009     Total estimated model params size (MB)\n",
      "2         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24:  21%|██▏       | 84/391 [00:00<00:03, 93.23it/s, v_num=yb8j]     "
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import itertools\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "import lightning as L\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from lightning.pytorch.callbacks import Callback\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "class PlotEmbeddingsCallback(Callback):\n",
    "    def __init__(self, log_dir: Path):\n",
    "        super().__init__()\n",
    "        self.log_dir = log_dir\n",
    "        self.log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def on_train_epoch_end(self, trainer: L.Trainer, pl_module: L.LightningModule):\n",
    "        embeddings = pl_module.embedding.weight.detach().cpu().numpy()\n",
    "\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.scatter(embeddings[:, 0], embeddings[:, 1])\n",
    "\n",
    "        for i in range(len(embeddings)):\n",
    "            plt.annotate(str(i), (embeddings[i, 0], embeddings[i, 1]))\n",
    "\n",
    "        plt.title(f\"Embedding Space at Epoch {trainer.current_epoch}\")\n",
    "        plt.xlabel(\"Dimension 1\")\n",
    "        plt.ylabel(\"Dimension 2\")\n",
    "\n",
    "        plt.savefig(self.log_dir / f\"embeddings_epoch_{trainer.current_epoch}.png\")\n",
    "        plt.close()\n",
    "\n",
    "        if isinstance(trainer.logger, WandbLogger):\n",
    "            trainer.logger.log_image(\n",
    "                key=\"embeddings\",\n",
    "                images=[self.log_dir / f\"embeddings_epoch_{trainer.current_epoch}.png\"],\n",
    "            )\n",
    "\n",
    "\n",
    "class TupleDataModule(L.LightningDataModule):\n",
    "    def __init__(self, tuple_size: int, range_size: int, batch_size: int):\n",
    "        super().__init__()\n",
    "        self.tuple_size = tuple_size\n",
    "        self.range_size = range_size\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.train_data = None\n",
    "        self.val_data = None\n",
    "        self.test_data = None\n",
    "\n",
    "    def setup(self, stage: str | None = None):\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            all_combinations = torch.tensor(\n",
    "                list(itertools.product(range(self.range_size), repeat=self.tuple_size))\n",
    "            )\n",
    "            indices = torch.randperm(len(all_combinations))\n",
    "\n",
    "            train_size = int(len(all_combinations) * 0.8)\n",
    "            train_indices = indices[:train_size]\n",
    "            val_indices = indices[train_size:]\n",
    "\n",
    "            train_tensor = all_combinations[train_indices]\n",
    "            val_tensor = all_combinations[val_indices]\n",
    "\n",
    "            self.train_data = TensorDataset(train_tensor)\n",
    "            self.val_data = TensorDataset(val_tensor)\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader[Tuple[torch.Tensor, ...]]:\n",
    "        assert self.train_data is not None\n",
    "        return DataLoader(\n",
    "            self.train_data, batch_size=self.batch_size, shuffle=True, num_workers=4\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader[Tuple[torch.Tensor, ...]]:\n",
    "        assert self.val_data is not None\n",
    "        return DataLoader(self.val_data, batch_size=self.batch_size, num_workers=4)\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader[Tuple[torch.Tensor, ...]]:\n",
    "        assert self.test_data is not None\n",
    "        return DataLoader(self.test_data, batch_size=self.batch_size, num_workers=4)\n",
    "\n",
    "\n",
    "class TupleAutoencoder(L.LightningModule):\n",
    "    def __init__(self, tuple_length: int, range_size: int, embedding_dim: int):\n",
    "        super().__init__()\n",
    "        self.tuple_length = tuple_length\n",
    "        self.range_size = range_size\n",
    "\n",
    "        self.embedding = nn.Embedding(range_size, embedding_dim)\n",
    "        self.linear = nn.Linear(tuple_length * embedding_dim, tuple_length * range_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        embedded = self.embedding(x)\n",
    "        embedded_flat = embedded.view(batch_size, -1)\n",
    "        output = self.linear(embedded_flat)\n",
    "        output = output.view(batch_size, self.tuple_length, self.range_size)\n",
    "        output = F.softmax(output, dim=-1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def _compute_loss(self, batch: torch.Tensor) -> torch.Tensor:\n",
    "        x = batch[0]\n",
    "        output = self(x)\n",
    "\n",
    "        loss = F.nll_loss(\n",
    "            output.log().view(-1, self.range_size), x.view(-1), reduction=\"sum\"\n",
    "        )\n",
    "        return loss / x.size(0)\n",
    "\n",
    "    def training_step(self, batch: torch.Tensor, batch_idx: int) -> torch.Tensor:\n",
    "        loss = self._compute_loss(batch)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: torch.Tensor, batch_idx: int):\n",
    "        loss = self._compute_loss(batch)\n",
    "        self.log(\"val_loss\", loss)\n",
    "\n",
    "    def configure_optimizers(self) -> torch.optim.Optimizer:\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "TUPLE_SIZE = 3\n",
    "RANGE_SIZE = 100\n",
    "EMBEDDING_DIM = 2\n",
    "BATCH_SIZE = 2048\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "datamodule = TupleDataModule(\n",
    "    tuple_size=TUPLE_SIZE, range_size=RANGE_SIZE, batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "model = TupleAutoencoder(TUPLE_SIZE, RANGE_SIZE, EMBEDDING_DIM)\n",
    "\n",
    "run_name = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "wandb_logger = WandbLogger(run_name, project=\"tuple-autoencoder\")\n",
    "\n",
    "log_dir = Path(\"logs\") / run_name\n",
    "plot_callback = PlotEmbeddingsCallback(log_dir)\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=NUM_EPOCHS,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    logger=wandb_logger,\n",
    "    callbacks=[plot_callback],\n",
    ")\n",
    "\n",
    "trainer.fit(model=model, datamodule=datamodule)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
